---
title: "BIO8068 Data Visualisaton in Ecology"
subtitle: Machine learning to classify images
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
load("animals.RData")
```

## Introduction

Machine learning can be used in a wide range of ecological applications. These include analyses of data from camera traps, acoustic analysis of birdsong, object detection to track animal movements etc. All the methods require large amounts of data, and the newest ones, implemented in Python libraries Tensorflow and Keras, work best with PCs that also have Graphics Processing Units (GPUs). One challenge is the _data bottleneck_. This is that the machine learning models must be trained on pre-classified data. So, for camera trap imagery data, this might require thousands of images that someone has manually identified to different species. Obviously, this process is slow and time-consuming. Once the model has been trained, it can be used on new sets of _similar_ data. It will fail if used with completely unrelated data.

## Examples used in this practical
We'll begin by using a standard, very large, dataset of photographs of cats and dogs, and train a convolutional neural network (CNN) to split them into 2 classes (a binary classification problem). Then we'll look at a much smaller dataset of multiple animal classes of wild and domestic species. A multi-class problem is more typical of what you might have to work on, but is computationally heavier, so I have only provided a small dataset for you. This means that the final accuracy of your CNN will be lower.

## Installing Keras and Tensorflow
You can find lots of useful information about these packages at <https://keras.rstudio.com/> which has many tutorials to take you from first steps. Classic datasets which you might find useful to learn more about deep learning are 'cats and dogs' (binary classification), 'mnist' (thousands of hand-drawn numbers from 0 to 9) and the 'fashion' dataset (simple outlines of shoes, socks, shirts etc.). See under the "Tutorials -> Basic classification" example on the Keras website. I also recommend the book "Deep learning in R" by Francois Chollet, which is available in the Robinson Library.

Installation is a little different from most R packages, as the deep learning is done by a Python 'back-end'. You need to issue the following commands, just once, to setup the system

```{r install keras, eval=FALSE}
# First use
install.packages("keras")
library(keras)
install_keras()

# All subsequent use, simply issue
# library(keras)
```

On all subsequent sessions you only need to issue the `library(keras)` command as you would in any other R package.

## Classification of animal photographs
Download the data file `animals.zip` from the Blackboard website. Copy it from your Downloads folder into a separate folder called `animals` for this practical, and unzip the file. You will see that it contains two sub-folders, `Training` and `Validation`, and within these are separate folders for each of four animal groups: butterfly, cow, elephant, spider. Obviously, these are very contrasting taxa and you don't really need machine learning to separate them, but we are working with a very small dataset so we want contrasts. In the `Training` dataset there are 80 photographs per taxon, whilst in the `Validation` dataset there are a different set of 20 photographs. This 80:20 split is quite a common one when developing a ML model. Sometimes people recommend adding another 20% of photographs to be used as a `Test` dataset after the ML has been created and trained, but for simplicity we are not doing that.

### Setup
Create an R project in your `animals` folder, with the two folders `Training` and `Validation` below. Begin by setting up the environment, attaching labels to your categories, and decide on the size of the images. The original images are typically 340 x 640 pixels in size, but they vary. They all need to be the same size for the analysis; if we make them too small we lose detail, but too large and the analysis will be slow.

```{r initial setup}
# list of animals to model
animal_list <- c("Butterfly", "Cow", "Elephant", "Spider")

# number of output classes (i.e. fruits)
output_n <- length(animal_list)

# image size to scale down to (original images vary but about 600 x 800 px)
img_width <- 250
img_height <- 250
target_size <- c(img_width, img_height)

# RGB = 3 channels
channels <- 3

# path to image folders
# Note: you may need to omit the trailing \\ if using MacOS or Linux
train_image_files_path <- "Training\\"
valid_image_files_path <- "Validation\\"
```

Notice in the above code that we have set `channels <- 3`. This is to account for Red-Green-Blue (RGB) full colour images. If you only have monochrome images (e.g. from night-time infra-red camera traps) you only have one channel. You should be able to confirm that there are the correct number of images in your dataset by issuing the command `length(list.files(train_image_files_path, recursive = TRUE))`, and similarly for the `Validation` folder with the `valid_image_files_path`.

### Data generators and augmentation
When you are working with small datasets it is useful to augment the training data. This means you can flip, zoom, transpose some of your training data to improve the accuracy. We will compare the results with and without augmentation. Note: *never* augment validation data. We begin without augmentation, and merely rescale all the pixel values to between 0 and 1 (photographs normally have a maximum pixel value for red, green or blue of 255).

```{r augmentation skeleton}
# Rescale from 255 to between zero and 1
train_data_gen = image_data_generator(
  rescale = 1/255
)

valid_data_gen <- image_data_generator(
  rescale = 1/255
)  
```

The `image_data_generator` function controls what happens to the images as they are read into R. At the moment all we are doing is rescaling the value of their pixels. The full import from the folders is done by the `flow_images_from_folder` function:

```{r data generator}
# training images
train_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                                    train_data_gen,
                                                    target_size = target_size,
                                                    class_mode = "categorical",
                                                    classes = animal_list,
                                                    seed = 42)

# validation images
valid_image_array_gen <- flow_images_from_directory(valid_image_files_path, 
                                                    valid_data_gen,
                                                    target_size = target_size,
                                                    class_mode = "categorical",
                                                    classes = animal_list,
                                                    seed = 42)
```

The above looks a little complicated, but it simply tells the machine learning algorithm where to read the files from (the file_path, one for Training, one for Validation), that the pixel values should be standardised to 0-1, their size is changed to 250 x 250 which we set earlier (stored in `target_size`). Machine learning can deal with continuous, binary and categorical data; we have multiple categories so we explain the setup. The actual data processing will be done by *Tensorflow* which is a *Python* rather than *R* package. This means that any random number generator seeds we set in R will not carry over, and we will get different results every time we train our model (which can be confusing), as the initial weights are usually randomised. By putting the random number seed (arbitrarily set at 42) into the data generator we minimise this problem.

Before you go any further, it is worth checking that the data generators have correctly read in the number of classes from each sub-folder and the names look OK. If nothing is returned, check your file and folder paths for typing errors.

```{r check generator}
# Check that things seem to have been read in OK
cat("Number of images per class:")
table(factor(train_image_array_gen$classes))
cat("Class labels vs index mapping")
train_image_array_gen$class_indices
```

Recall that `keras` calls an external machine-learning library called Tensorflow, which is not written in R, but a different language called Python. One oddity with Python is that it counts values in an object from zero, rather than one. This is why the Butterfly class has been coded as 0, and the maximum class of 3 (rather than 4) is allocated to spider.

### Final setup
We just want to add a few more items before we can define our machine learning model

```{r final setup}
# number of training samples
train_samples <- train_image_array_gen$n
# number of validation samples
valid_samples <- valid_image_array_gen$n

# define batch size and number of epochs
batch_size <- 32 # Typical default, though possibly a little high given small dataset
epochs <- 10
```

The numbers of training and validation samples is fairly self-explanatory, and you can double-check the values look OK. The images are pushed through the machine learning algorithm in "batches", rather than all at once, as this slightly speeds up data processing. The model takes a while to train, and initially is very inaccurate. As each round of training, or "epoch" is completed, it improves in accuracy. Depending on the complexity of your problem, you might need 20 or 30 epochs.

## Design your convolutional neural network
Now you define how your CNN is structured.

```{r define CNN structure}
# initialise model
model <- keras_model_sequential()

# add layers
model %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), input_shape = c(img_width, img_height, channels), activation = "relu") %>%

  # Second hidden layer
  layer_conv_2d(filter = 16, kernel_size = c(3,3), activation = "relu") %>%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %>%
  layer_dense(100, activation = "relu") %>%
  layer_dropout(0.5) %>%
  
  # Outputs from dense layer are projected onto output layer
  layer_dense(output_n, activation = "softmax") 
```

*Note:* there are other ways in which an identical CNN to that shown above could be written. For example, instead of including `activation = "relu"` in the `layer_dense` lines, you could pipe it into the line below, using the `layer_activation` function. The above defines a model with 2 convolution layers, one max-pooling layer (see lecture), and then this is flattened (`layer_flatten`) into a conventional neural network (`layer_dense`) with `output_n` output classes. A few key points:

* begin with convolutional and pooling layers, that gradually get smaller (32 and 16 above)
* use `relu` as the activation until the end, when `softmax` is used as we have categorical data
* the number of output classes, `output_n` has to equal the number of categories
* `layer_dropout` is used to randomly omit links in your CNN, which can improve accuracy
* Keras models are modified "in place". Hence, we went `model %>%` rather than `model <- ` above

It is useful to check the CNN structure before compiling and running it

```{r check CNN structure}
print(model)
```

### Compile and train the model
Now at last you are read to "compile" the model and train it. As it trains, an interactive graph will appear in the viewer, showing the accuracy, which will increase, and the loss, which will decrease. Ideally both the training and validation accuracy increase together, but if the latter reaches a plateau then it indicates a risk of overtraining. First you need to "compile" your CNN model:

```{r compile}
# Compile the model
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 0.0001, decay = 1e-6),
  metrics = "accuracy"
)
```

Compiling a model simply defines what error term to use to measure the accuracy of the model. With separate classes, use `categorical_crossentropy` but if, for example, you had presence/absence data use `binary_crossentropy`. Now you can train it. Depending on the CPU on your PC, the next step will take about 5 minutes to complete.

```{r train model, eval=FALSE}
# Train the model with fit_generator
history <- model %>% fit(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # print progress
  verbose = 2
)
```

The results of the model training are stored in the `history` object, which can be plotted, and it is useful to save your model. For something small, like this example, you can simple save the R data space with the `save.image` command. For larger models, especially if you are fine-tuning them and want to compare outputs and predictions, it is better to use the dedicated Keras `save_model_hdf5` which stores it in a special hdf5 format. You can retrieve a model using the `load_model_hdf5` command.

```{r plot and save results, eval=FALSE}
plot(history)
save.image("animals.RData")
model %>% save_model_hdf5("animals_simple.hdf5")
```

```{r, echo=FALSE}
plot(history)
```


Finally, to see how well the model predicts individual photos, you can use the `predict_classes` function. Remember from earlier, that the index for the first class (Butterfly) is zero, so if the model is working then most of the first set should be zeros etc.

## Improving model predictions with data augmentation
Your model probably gives fairly low prediction levels, of e.g. 50 to 65% accuracy, but given the very small size of the dataset this is actually quite good. Keep in mind that with 4 classes the random level of accuracy would be 25%. Let's see if you can improve the model accuracy by using data augmentation. Change the original training data generator to:

```{r augmented generator}
train_data_gen = image_data_generator(
  rescale = 1/255 ,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)
train_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                                    train_data_gen,
                                                    target_size = target_size,
                                                    class_mode = "categorical",
                                                    classes = animal_list,
                                                    seed = 42)
```

Now refit the model, and see what the results are like:

```{r fit augmented model, eval=FALSE}
# Copy the original model (this is not essential; could keep the same name)
model_aug <- model

# Train the model with fit_generator
history_aug <- model_aug %>% fit(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # print progress
  verbose = 2
)

plot(history_aug)
```
```{r, echo=FALSE}
plot(history_aug)
```


If you are lucky (!) the model accuracy might have improved (it didn't for me I will admit).

To understand what augmentation is doing, it can be useful to look at just one image, the original, and augmented versions.

```{r one augmented image}
fnames <- list.files(train_image_files_path, full.names = TRUE, recursive = TRUE)
# Pick the third image (a butterfly) to look at
img_path <- fnames[[3]]                                               

img <- image_load(img_path, target_size = c(150, 150))                
img_array <- image_to_array(img)                                      
img_array <- array_reshape(img_array, c(1, 150, 150, 3))              

# Very simple augmentation generator
augmentation_generator <- flow_images_from_data(                      
  img_array,                                                          
  generator = train_data_gen,                                                
  batch_size = 1
)

op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))            
for (i in 1:4) {                                                      
  batch <- generator_next(augmentation_generator)                     
  plot(as.raster(batch[1,,,]))                                        
}                                                                     
par(op)                                                               

```


You can see how the images have been flipped horizontally (but not vertically, as we did not request that), and slightly shifted and zoomed.

### In your own time
Try out tweaking some of the parameters for data augmentation, batch sizes, image rescaling, to see if they improve the quality of your image classifications.